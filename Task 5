import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay

# Load Dataset
df = pd.read_csv("C:/Users/NAVADEEP/OneDrive/Pictures/Documents/Main flow/Task 5/student pass-fail.csv")

# Select relevant columns and rename
df = df[["Total Hours in Module Area", "Percent Attended", "label (fail=1, pass=0)"]]
df.columns = ["StudyHours", "Attendance", "Pass"]
# Drop rows with missing values
df.dropna(inplace=True)
# Data Exploration
# First five rows of dataset
print(df.head())
# Check for outliers and distribution
print(df.describe())
# Drop rows with missing values
df.dropna(inplace=True)

# Scatter Plot
plt.figure(figsize=(8, 5))
sns.scatterplot(data=df, x="StudyHours", y="Attendance", hue="Pass", palette="coolwarm")
plt.title("Study Hours vs Attendance by Pass/Fail")
plt.show()

# Model Training
# Features and target
X = df[["StudyHours", "Attendance"]]
y = df["Pass"]

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression
model = LogisticRegression()
model.fit(X_train, y_train)

# 8 Model Evaluation
# Predictions
y_pred = model.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Pass", "Fail"])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix")
plt.show()import pandas as pd
import numpy as np
import re
import string
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Download necessary resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# 2. Load Dataset
df = pd.read_csv("C:/Users/NAVADEEP/OneDrive/Pictures/Documents/Main flow/Task 5/reviews.csv") 
print(df.head())
df = df[["review", "sentiment"]]df.dropna(inplace=True)

# 3. Preprocessing Function
def preprocess(text):
    # Lowercase
    text = text.lower()
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    # Remove punctuation and special characters
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [w for w in tokens if w not in stop_words]
    # Lemmatize
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return ' '.join(tokens)

# Apply preprocessing
df['cleaned_review'] = df['review'].apply(preprocess)

# 4. Vectorization using TF-IDF
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['cleaned_review'])

# 5. Convert Target to Binary
df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})
y = df['sentiment']# 6. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. Model Training
model = LogisticRegression()
model.fit(X_train, y_train)

# 8. Model Evaluation
y_pred = model.predict(X_test)

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Model Evaluation Report:")
print(f"Accuracy  : {accuracy:.2f}")
print(f"Precision : {precision:.2f}")
print(f"Recall    : {recall:.2f}")
print(f"F1-Score  : {f1:.2f}")
print("\nDetailed Report:\n")
print(classification_report(y_test, y_pred, target_names=["Negative", "Positive"]))
# 9. Insights â€“ View Examples
df_test = df.iloc[y_test.index]
df_test = df_test.assign(Predicted=y_pred)

# Correct Predictions
correct = df_test[df_test['sentiment'] == df_test['Predicted']].head(3)

# Incorrect Predictions
incorrect = df_test[df_test['sentiment'] != df_test['Predicted']].head(3)

print("\nCorrectly Classified Examples:")
for i, row in correct.iterrows():
    print(f"\nReview: {row['review']}\nPredicted Sentiment: {row['Predicted']}")

print("\nIncorrectly Classified Examples:")
for i, row in incorrect.iterrows():
    print(f"\nReview: {row['review']}\nActual Sentiment: {row['sentiment']} | Predicted: {row['Predicted']}")
